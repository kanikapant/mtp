diff -Nar -U 5 kernel_source/original/linux-4.1.3/drivers/virtio/Kconfig kernel_source/patched/linux-4.1.3/drivers/virtio/Kconfig
--- kernel_source/original/linux-4.1.3/drivers/virtio/Kconfig	2015-09-20 01:21:37.402585000 +0530
+++ kernel_source/patched/linux-4.1.3/drivers/virtio/Kconfig	2015-08-05 18:23:19.000282000 +0530
@@ -31,11 +31,11 @@
 	  If disabled, you get a slightly smaller, non-transitional driver,
 	  with no legacy compatibility.
 
           So look out into your driveway.  Do you have a flying car?  If
           so, you can happily disable this option and virtio will not
-          break.  Otherwise, leave it set.  Unless you're testing what
+          break.  Otherwise, leave it set.  Unless you are testing what
           life will be like in The Future.
 
 	  If unsure, say Y.
 
 config VIRTIO_BALLOON
@@ -77,6 +77,14 @@
 	 address in particular) can crash your system - you have been warned.
 	 See Documentation/kernel-parameters.txt for details.
 
 	 If unsure, say 'N'.
 
+#ABY MTP#
+config KVM_TMEM
+	bool "KVM guest tmem support"
+	default y
+	---help---
+	This option enables support for KVM guest transcendant memory 
+	to use KVM host level tmem features.
+#END ABY MTP#
 endmenu
diff -Nar -U 5 kernel_source/original/linux-4.1.3/drivers/virtio/Makefile kernel_source/patched/linux-4.1.3/drivers/virtio/Makefile
--- kernel_source/original/linux-4.1.3/drivers/virtio/Makefile	2015-09-20 01:21:37.402585000 +0530
+++ kernel_source/patched/linux-4.1.3/drivers/virtio/Makefile	2015-08-05 18:23:19.000282000 +0530
@@ -3,5 +3,8 @@
 obj-$(CONFIG_VIRTIO_PCI) += virtio_pci.o
 virtio_pci-y := virtio_pci_modern.o virtio_pci_common.o
 virtio_pci-$(CONFIG_VIRTIO_PCI_LEGACY) += virtio_pci_legacy.o
 obj-$(CONFIG_VIRTIO_BALLOON) += virtio_balloon.o
 obj-$(CONFIG_VIRTIO_INPUT) += virtio_input.o
+#ABY MTP#
+obj-$(CONFIG_KVM_TMEM) += tmem.o
+#END ABY MTP#
diff -Nar -U 5 kernel_source/original/linux-4.1.3/drivers/virtio/tmem.c kernel_source/patched/linux-4.1.3/drivers/virtio/tmem.c
--- kernel_source/original/linux-4.1.3/drivers/virtio/tmem.c	1970-01-01 05:30:00.000000000 +0530
+++ kernel_source/patched/linux-4.1.3/drivers/virtio/tmem.c	2015-08-26 13:27:03.945976000 +0530
@@ -0,0 +1,553 @@
+
+/*
+ * Tmem implementation for transcendent memory (tmem)
+ *
+ * Copyright (C) 2009-2011 Oracle Corp.  All rights reserved.
+ * Author: Dan Magenheimer
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/init.h>
+#include <linux/pagemap.h>
+#include <linux/cleancache.h>
+#include <linux/kvm_para.h>
+
+#include "tmem.h"
+/* temporary ifdef until include/linux/frontswap.h is upstream */
+#ifdef CONFIG_FRONTSWAP
+#include <linux/frontswap.h>
+#endif
+
+
+/* kvm tmem foundation ops/hypercalls */
+static inline int kvm_hcall_tmem_op(struct kvm_tmem_op *ptr)
+{
+  unsigned long addr = __pa((unsigned long)ptr);
+  //int hcall_no = KVM_TMEM_HCALL;
+	/*ABY MTP*/
+  	/*
+   	 * Uncomment this if you are ready to fill your hdd space with log info!!
+	 pr_info("MTP | CALL: kvm_hcall_tmem_op | CALLED BY: kvm_tmem_op |\n");
+	 */
+	/*ABY END MTP*/
+  return kvm_hypercall1(KVM_TMEM_HCALL,addr);
+ /* __asm__("movl %0, %eax;");
+  __asm__("mov %1, %%rbx;"
+           :
+           :"r"(hcall_no),"r"(addr)
+           :"%rbx");
+  __asm__ ("vmcall");*/
+
+}
+static inline int kvm_tmem_op(u32 tmem_cmd, u32 tmem_pool, struct tmem_oid oid,
+	u32 index, unsigned long gmfn, u32 tmem_offset, u32 pfn_offset, u32 len)
+{
+	struct kvm_tmem_op op;
+	int rc = 0;
+	
+	/*ABY MTP*/
+	/*
+	 * Uncomment only if you are ready to fill up your hdd space with log info!!
+	char tmem_op_str[50];
+	switch (tmem_cmd)
+	{
+		case TMEM_CONTROL:
+			snprintf(tmem_op_str, sizeof(tmem_op_str), "tmem_control");
+			break;
+		case TMEM_NEW_POOL:
+			snprintf(tmem_op_str, sizeof(tmem_op_str), "tmem_cleancache_init_fs");
+			break;
+		case TMEM_DESTROY_POOL:
+			snprintf(tmem_op_str, sizeof(tmem_op_str), "tmem_cleancache_flush_fs");
+			break;
+		case TMEM_NEW_PAGE:
+			snprintf(tmem_op_str, sizeof(tmem_op_str), "tmem_new_page");
+			break;
+		case TMEM_PUT_PAGE:
+			snprintf(tmem_op_str, sizeof(tmem_op_str), "tmem_cleancache_put_page");
+			break;
+		case TMEM_GET_PAGE:
+			snprintf(tmem_op_str, sizeof(tmem_op_str), "tmem_cleancache_get_page");
+			break;
+		case TMEM_FLUSH_PAGE:
+			snprintf(tmem_op_str, sizeof(tmem_op_str), "tmem_cleancache_flush_page");
+			break;
+		case TMEM_FLUSH_OBJECT:
+			snprintf(tmem_op_str, sizeof(tmem_op_str), "tmem_cleancache_flush_inode");
+			break;
+		case TMEM_READ:
+			snprintf(tmem_op_str, sizeof(tmem_op_str), "tmem_read");
+			break;
+		case TMEM_WRITE:
+			snprintf(tmem_op_str, sizeof(tmem_op_str), "tmem_write");
+			break;
+		case TMEM_XCHG:
+			snprintf(tmem_op_str, sizeof(tmem_op_str), "tmem_exchg");
+			break;
+		default:
+			snprintf(tmem_op_str, sizeof(tmem_op_str), "NO TMEM OP");
+	}
+	printk("MTP | CALL: kvm_tmem_op | CALLED BY: %s |\n", tmem_op_str);
+	 */
+	/*ABY END MTP*/
+	op.cmd = tmem_cmd;
+	op.pool_id = tmem_pool;
+	op.u.gen.oid[0] = oid.oid[0];
+	op.u.gen.oid[1] = oid.oid[1];
+	op.u.gen.oid[2] = oid.oid[2];
+	op.u.gen.index = index;
+	op.u.gen.tmem_offset = tmem_offset;
+	op.u.gen.pfn_offset = pfn_offset;
+	op.u.gen.len = len;
+	op.u.gen.gmfn = gmfn;
+	rc = kvm_hcall_tmem_op(&op);
+	return rc;
+}
+
+static int kvm_tmem_new_pool(struct tmem_pool_uuid uuid,
+				u32 flags, unsigned long pagesize)
+{
+	struct kvm_tmem_op op;
+	int rc = 0, pageshift;
+	/*ABY MTP*/
+	/*
+	 * Uncomment only if you are ready to fill up your hdd space with log info!!
+	printk("MTP | CALL: kvm_tmem_new_pool | CALLED BY: tmem_cleancache_init_fs |\n");
+	 */
+	/*ABY END MTP*/
+	for (pageshift = 0; pagesize != 1; pageshift++)
+		pagesize >>= 1;
+	flags |= (pageshift - 12) << TMEM_POOL_PAGESIZE_SHIFT;
+	flags |= TMEM_SPEC_VERSION << TMEM_VERSION_SHIFT;
+	op.cmd = TMEM_NEW_POOL;
+	op.u.new.uuid[0] = uuid.uuid_lo;
+	op.u.new.uuid[1] = uuid.uuid_hi;
+	op.u.new.flags = flags;
+	rc = kvm_hcall_tmem_op(&op);
+	return rc;
+}
+
+/* kvm generic tmem ops */
+
+static int kvm_tmem_put_page(u32 pool_id, struct tmem_oid oid,
+			     u32 index, unsigned long pfn)
+{
+	unsigned long gmfn = pfn;
+	/*ABY MTP*/
+	/*
+	 * Uncomment only if you are ready to fill up your hdd space with log info!!
+	printk("MTP | CALL: kvm_tmem_put_page | CALLED BY: tmem_cleancache_put_page |\n");
+	 */
+	/*ABY END MTP*/
+	return kvm_tmem_op(TMEM_PUT_PAGE, pool_id, oid, index,
+		gmfn, 0, 0, 0);
+}
+
+static int kvm_tmem_get_page(u32 pool_id, struct tmem_oid oid,
+			     u32 index, unsigned long pfn)
+{
+	unsigned long gmfn = pfn;
+	/*ABY MTP*/
+	/*
+	 * Uncomment only if you are ready to fill up your hdd space with log info!!
+	printk("MTP | CALL: kvm_tmem_get_page | CALLED BY: tmem_cleancache_get_page |\n");
+	 */
+	/*ABY END MTP*/
+	return kvm_tmem_op(TMEM_GET_PAGE, pool_id, oid, index,
+		gmfn, 0, 0, 0);
+}
+
+static int kvm_tmem_flush_page(u32 pool_id, struct tmem_oid oid, u32 index)
+{
+	/*ABY MTP*/
+	/*
+	 * Uncomment only if you are ready to fill up your hdd space with log info!!
+	printk("MTP | CALL: kvm_tmem_flush_page | CALLED BY: tmem_cleancache_flush_page |\n");
+	 */
+	/*ABY END MTP*/
+	return kvm_tmem_op(TMEM_FLUSH_PAGE, pool_id, oid, index,
+		0, 0, 0, 0);
+}
+
+static int kvm_tmem_flush_object(u32 pool_id, struct tmem_oid oid)
+{
+	/*ABY MTP*/
+	/*
+	 * Uncomment only if you are ready to fill up your hdd space with log info!!
+	printk("MTP | CALL: kvm_tmem_flush_object | CALLED BY: tmem_cleancache_flush_inode |\n");
+	 */
+	/*ABY END MTP*/
+	return kvm_tmem_op(TMEM_FLUSH_OBJECT, pool_id, oid, 0, 0, 0, 0, 0);
+}
+
+int kvm_tmem_enabled __read_mostly;
+EXPORT_SYMBOL(kvm_tmem_enabled);
+
+static int __init enable_tmem(char *s)
+{
+	kvm_tmem_enabled = 1;
+	return 0;
+}
+
+early_param("kvmtmem", enable_tmem);
+
+#ifdef CONFIG_CLEANCACHE
+static int kvm_tmem_destroy_pool(u32 pool_id)
+{
+	struct tmem_oid oid = { { 0 } };
+	/*ABY MTP*/
+	/*
+	 * Uncomment only if you are ready to fill up your hdd space with log info!!
+	printk("MTP | CALL: kvm_tmem_destroy_pool | CALLED BY: tmem_cleancache_flush_fs |\n");
+	 */
+	/*ABY END MTP*/
+	return kvm_tmem_op(TMEM_DESTROY_POOL, pool_id, oid, 0, 0, 0, 0, 0);
+}
+
+/* cleancache ops */
+
+static int tmem_cleancache_put_page(int pool, struct cleancache_filekey key,
+				     pgoff_t index, struct page *page)
+{
+	u32 ind = (u32) index;
+	struct tmem_oid oid = *(struct tmem_oid *)&key;
+	unsigned long pfn = page_to_pfn(page);
+	/*ABY MTP*/
+	/*
+	 * Uncomment only if you are ready to fill up your hdd space with log info!!
+	printk("MTP | CALL: tmem_cleancache_put_page | CALLED BY: cleancache_put_page |\n");
+	 */
+	/*ABY END MTP*/
+	if (pool < 0)
+		return -1;
+	if (ind != index)
+		return -1;
+	mb(); /* ensure page is quiescent; tmem may address it with an alias */
+	return kvm_tmem_put_page((u32)pool, oid, ind, pfn);
+}
+
+static int tmem_cleancache_get_page(int pool, struct cleancache_filekey key,
+				    pgoff_t index, struct page *page)
+{
+	u32 ind = (u32) index;
+	struct tmem_oid oid = *(struct tmem_oid *)&key;
+	unsigned long pfn = page_to_pfn(page);
+	int ret;
+	/*ABY MTP*/
+	/*
+	 * Uncomment only if you are ready to fill up your hdd space with log info!!
+	printk("MTP | CALL: tmem_cleancache_get_page | CALLED BY: cleancache_get_page |\n");
+	 */
+	/*ABY END MTP*/
+	/* translate return values to linux semantics */
+	if (pool < 0)
+		return -1;
+	if (ind != index)
+		return -1;
+	ret = kvm_tmem_get_page((u32)pool, oid, ind, pfn);
+	if (ret == 0)
+		return 0;
+	else
+		return -1;
+}
+
+static int tmem_cleancache_flush_page(int pool, struct cleancache_filekey key,
+				       pgoff_t index)
+{
+	u32 ind = (u32) index;
+	struct tmem_oid oid = *(struct tmem_oid *)&key;
+	/*ABY MTP*/
+	/*
+	 * Uncomment only if you are ready to fill up your hdd space with log info!!
+	printk("MTP | CALL: tmem_cleancache_flush_page | CALLED BY: cleancache_invalidate_page |\n");
+	*/
+	/*ABY END MTP*/
+	if (pool < 0)
+		return -1;
+	if (ind != index)
+		return -1;
+	return kvm_tmem_flush_page((u32)pool, oid, ind);
+}
+
+static void tmem_cleancache_flush_inode(int pool, struct cleancache_filekey key)
+{
+	struct tmem_oid oid = *(struct tmem_oid *)&key;
+	/*ABY MTP*/
+	/*
+	 * Uncomment only if you are ready to fill up your hdd space with log info!!
+	printk("MTP | CALL: tmem_cleancache_flush_inode | CALLED BY: cleancache_invalidate_inode |\n");
+	 */
+	/*ABY END MTP*/
+
+	if (pool < 0)
+		return;
+	(void)kvm_tmem_flush_object((u32)pool, oid);
+}
+
+static void tmem_cleancache_flush_fs(int pool)
+{
+	/*ABY MTP*/
+	/*
+	 * Uncomment only if you are ready to fill up your hdd space with log info!!
+	printk("MTP | CALL: tmem_cleancache_flush_fs | CALLED BY: cleancache_invalidate_fs |\n");
+	 */
+	/*ABY END MTP*/
+	if (pool < 0)
+		return;
+	(void)kvm_tmem_destroy_pool((u32)pool);
+}
+
+static int tmem_cleancache_init_fs(size_t pagesize)
+{
+	struct tmem_pool_uuid uuid_private = TMEM_POOL_PRIVATE_UUID;
+	/*ABY MTP*/
+	/*
+	 * Uncomment only if you are ready to fill up your hdd space with log info!!
+	printk("MTP | CALL: tmem_cleancache_init_fs | CALLED BY: cleancache_init_fs |\n");
+	 */
+	/*ABY END MTP*/
+	return kvm_tmem_new_pool(uuid_private, 0, pagesize);
+}
+
+static int tmem_cleancache_init_shared_fs(char *uuid, size_t pagesize)
+{
+	struct tmem_pool_uuid shared_uuid;
+	/*ABY MTP*/
+	/*
+	 * Uncomment only if you are ready to fill up your hdd space with log info!!
+	printk("MTP | CALL: tmem_cleancache_init_shared_fs | CALLED BY: cleancache_init_shared_fs |\n");
+	 */
+	/*ABY END MTP*/
+
+	shared_uuid.uuid_lo = *(u64 *)uuid;
+	shared_uuid.uuid_hi = *(u64 *)(&uuid[8]);
+	return kvm_tmem_new_pool(shared_uuid, TMEM_POOL_SHARED, pagesize);
+}
+
+static int use_cleancache = 1;
+
+static int __init no_cleancache(char *s)
+{
+	use_cleancache = 0;
+	return 0;
+}
+
+__setup("nocleancache", no_cleancache);
+
+static struct cleancache_ops tmem_cleancache_ops = {
+	.put_page = tmem_cleancache_put_page,
+	.get_page = tmem_cleancache_get_page,
+	.invalidate_page = tmem_cleancache_flush_page,
+	.invalidate_inode = tmem_cleancache_flush_inode,
+	.invalidate_fs = tmem_cleancache_flush_fs,
+	.init_shared_fs = tmem_cleancache_init_shared_fs,
+	.init_fs = tmem_cleancache_init_fs
+	/*ABY MTP*/
+	/*
+	 * .flush_page = tmem_cleancache_flush_page,
+	 * .flush_inode = tmem_cleancache_flush_inode,
+	 * .flush_fs = tmem_cleancache_flush_fs,
+	 */
+	/*END ABY MTP*/
+};
+#endif
+
+#ifdef CONFIG_FRONTSWAP
+/* frontswap tmem operations */
+
+/* a single tmem poolid is used for all frontswap "types" (swapfiles) */
+static int tmem_frontswap_poolid;
+
+/*
+ * Swizzling increases objects per swaptype, increasing tmem concurrency
+ * for heavy swaploads.  Later, larger nr_cpus -> larger SWIZ_BITS
+ */
+#define SWIZ_BITS		4
+#define SWIZ_MASK		((1 << SWIZ_BITS) - 1)
+#define _oswiz(_type, _ind)	((_type << SWIZ_BITS) | (_ind & SWIZ_MASK))
+#define iswiz(_ind)		(_ind >> SWIZ_BITS)
+
+static inline struct tmem_oid oswiz(unsigned type, u32 ind)
+{
+	struct tmem_oid oid = { .oid = { 0 } };
+	oid.oid[0] = _oswiz(type, ind);
+	return oid;
+}
+
+/* returns 0 if the page was successfully put into frontswap, -1 if not */
+static int tmem_frontswap_put_page(unsigned type, pgoff_t offset,
+				   struct page *page)
+{
+	u64 ind64 = (u64)offset;
+	u32 ind = (u32)offset;
+	unsigned long pfn = page_to_pfn(page);
+	int pool = tmem_frontswap_poolid;
+	int ret;
+
+	if (pool < 0)
+		return -1;
+	if (ind64 != ind)
+		return -1;
+	mb(); /* ensure page is quiescent; tmem may address it with an alias */
+	ret = kvm_tmem_put_page(pool, oswiz(type, ind), iswiz(ind), pfn);
+	/* translate Xen tmem return values to linux semantics */
+	if (ret == 1)
+		return 0;
+	else
+		return -1;
+}
+
+/*
+ * returns 0 if the page was successfully gotten from frontswap, -1 if
+ * was not present (should never happen!)
+ */
+static int tmem_frontswap_get_page(unsigned type, pgoff_t offset,
+				   struct page *page)
+{
+	u64 ind64 = (u64)offset;
+	u32 ind = (u32)offset;
+	unsigned long pfn = page_to_pfn(page);
+	int pool = tmem_frontswap_poolid;
+	int ret;
+
+	if (pool < 0)
+		return -1;
+	if (ind64 != ind)
+		return -1;
+	ret = kvm_tmem_get_page(pool, oswiz(type, ind), iswiz(ind), pfn);
+	/* translate Xen tmem return values to linux semantics */
+	if (ret == 1)
+		return 0;
+	else
+		return -1;
+}
+
+/* flush a single page from frontswap */
+static void tmem_frontswap_flush_page(unsigned type, pgoff_t offset)
+{
+	u64 ind64 = (u64)offset;
+	u32 ind = (u32)offset;
+	int pool = tmem_frontswap_poolid;
+
+	if (pool < 0)
+		return;
+	if (ind64 != ind)
+		return;
+	(void) kvm_tmem_flush_page(pool, oswiz(type, ind), iswiz(ind));
+}
+
+/* flush all pages from the passed swaptype */
+static void tmem_frontswap_flush_area(unsigned type)
+{
+	int pool = tmem_frontswap_poolid;
+	int ind;
+
+	if (pool < 0)
+		return;
+
+	for (ind = SWIZ_MASK; ind >= 0; ind--)
+		(void)kvm_tmem_flush_object(pool, oswiz(type, ind));
+}
+
+static void tmem_frontswap_init(unsigned ignored)
+{
+	struct tmem_pool_uuid private = TMEM_POOL_PRIVATE_UUID;
+
+	/* a single tmem poolid is used for all frontswap "types" (swapfiles) */
+	if (tmem_frontswap_poolid < 0)
+		tmem_frontswap_poolid =
+		    kvm_tmem_new_pool(private, TMEM_POOL_PERSIST, PAGE_SIZE);
+}
+
+static int __initdata use_frontswap = 1;
+
+static int __init no_frontswap(char *s)
+{
+	use_frontswap = 0;
+	return 0;
+}
+
+__setup("nofrontswap", no_frontswap);
+
+static struct frontswap_ops tmem_frontswap_ops = {
+	/*ABY MTP*/
+	/*
+	 * .put_page = tmem_frontswap_put_page,
+	 * .get_page = tmem_frontswap_get_page,
+	 * .flush_page = tmem_frontswap_flush_page,
+	 * .flush_area = tmem_frontswap_flush_area,
+	 * .init = tmem_frontswap_init
+	 */
+	.init = tmem_frontswap_init,
+	.store = tmem_frontswap_put_page,
+	.load = tmem_frontswap_get_page,
+	.invalidate_page = tmem_frontswap_flush_page,
+	.invalidate_area = tmem_frontswap_flush_area
+	/*END ABY MTP*/
+
+};
+#endif
+
+static int __init kvm_tmem_init(void)
+{
+
+#ifdef CONFIG_FRONTSWAP
+	/*ABY MTP*/
+	printk("MTP | CALL: kvm_tmem_init | kvm_tmem_enabled: %d | use_frontswap: %d |\n",kvm_tmem_enabled, use_frontswap);
+	/*ABY END MTP*/
+	if (kvm_tmem_enabled && use_frontswap && 0) {
+		char *s = "";
+		/*ABY MTP*/
+		/*
+		 *struct frontswap_ops old_ops =
+		 */
+		 struct frontswap_ops* old_ops = frontswap_register_ops(&tmem_frontswap_ops);
+		tmem_frontswap_poolid = -1;
+
+		/*ABY MTP*/
+		/*
+		 * if (old_ops.init != NULL)
+		 */
+		 if (old_ops->init != NULL)
+			s = " (WARNING: frontswap_ops overridden)";
+		/*
+		 * printk(KERN_INFO "frontswap enabled, RAM provided by "
+		 * 		     		"XEN Transcendent Memory: %s\n", s);
+		 */
+		 printk(KERN_INFO "frontswap enabled, RAM provided by "
+		  		     		"KVM Transcendent Memory: %s\n", s);
+
+		 /*END ABY MTP*/
+	}
+#endif
+
+#ifdef CONFIG_CLEANCACHE
+	BUG_ON(sizeof(struct cleancache_filekey) != sizeof(struct tmem_oid));
+	/*ABY MTP*/
+	printk("MTP | CALL: kvm_tmem_init | kvm_tmem_enabled: %d | use_cleancache: %d |\n",kvm_tmem_enabled, use_cleancache );
+	/*ABY END MTP*/
+	if (kvm_tmem_enabled && use_cleancache) {
+		char *s = "";
+		/*ABY MTP*/
+		/*
+		 * struct cleancache_ops* old_ops =
+		 * cleancache_register_ops(&tmem_cleancache_ops);
+		 * if (old_ops->init_fs != NULL)
+		 */
+		 if(cleancache_register_ops(&tmem_cleancache_ops) == 0)
+			s = " (WARNING: cleancache_ops overridden)";
+		/*
+		 * printk(KERN_INFO "cleancache enabled, RAM provided by "
+		 * 		     		"XEN Transcendent Memory: %s\n", s);
+		 */
+		printk(KERN_INFO "cleancache enabled, RAM provided by "
+				 "KVM Transcendent Memory: %s\n", s);
+		/*END ABY MTP*/
+	}
+#endif
+	return 0;
+}
+
+module_init(kvm_tmem_init)
diff -Nar -U 5 kernel_source/original/linux-4.1.3/drivers/virtio/tmem.h kernel_source/patched/linux-4.1.3/drivers/virtio/tmem.h
--- kernel_source/original/linux-4.1.3/drivers/virtio/tmem.h	1970-01-01 05:30:00.000000000 +0530
+++ kernel_source/patched/linux-4.1.3/drivers/virtio/tmem.h	2015-08-26 13:25:56.357976000 +0530
@@ -0,0 +1,59 @@
+#ifndef __TMEM_H_
+#define __TMEM_H_
+
+
+#define TMEM_CONTROL               0
+#define TMEM_NEW_POOL              1
+#define TMEM_DESTROY_POOL          2
+#define TMEM_NEW_PAGE              3
+#define TMEM_PUT_PAGE              4
+#define TMEM_GET_PAGE              5
+#define TMEM_FLUSH_PAGE            6
+#define TMEM_FLUSH_OBJECT          7
+#define TMEM_READ                  8
+#define TMEM_WRITE                 9
+#define TMEM_XCHG                 10
+
+/* Bits for HYPERVISOR_tmem_op(TMEM_NEW_POOL) */
+#define TMEM_POOL_PERSIST          1
+#define TMEM_POOL_SHARED           2
+#define TMEM_POOL_PAGESIZE_SHIFT   4
+#define TMEM_VERSION_SHIFT        24
+
+
+struct tmem_pool_uuid {
+	u64 uuid_lo;
+	u64 uuid_hi;
+};
+
+struct tmem_oid {
+	u64 oid[3];
+};
+
+#define TMEM_POOL_PRIVATE_UUID	{ 0, 0 }
+
+/* flags for tmem_ops.new_pool */
+#define TMEM_POOL_PERSIST          1
+#define TMEM_POOL_SHARED           2
+#define KVM_TMEM_HCALL 10
+
+struct kvm_tmem_op {
+        uint32_t cmd;
+        int32_t pool_id;
+        union {
+                struct {  /* for cmd == TMEM_NEW_POOL */
+                        uint64_t uuid[2];
+                        uint32_t flags;
+                } new;
+                struct {
+                        uint64_t oid[3];
+                        uint32_t index;
+                        uint32_t tmem_offset;
+                        uint32_t pfn_offset;
+                        uint32_t len;
+                        unsigned long gmfn; /* guest machine page frame */
+                } gen;
+        } u;
+};
+
+#endif
diff -Nar -U 5 kernel_source/original/linux-4.1.3/drivers/xen/tmem.c kernel_source/patched/linux-4.1.3/drivers/xen/tmem.c
--- kernel_source/original/linux-4.1.3/drivers/xen/tmem.c	2015-09-20 01:21:12.438585000 +0530
+++ kernel_source/patched/linux-4.1.3/drivers/xen/tmem.c	2015-08-26 12:41:20.901976000 +0530
@@ -166,23 +166,23 @@
 	return xen_tmem_op(TMEM_DESTROY_POOL, pool_id, oid, 0, 0, 0, 0, 0);
 }
 
 /* cleancache ops */
 
-static void tmem_cleancache_put_page(int pool, struct cleancache_filekey key,
-				     pgoff_t index, struct page *page)
+static int tmem_cleancache_put_page(int pool, struct cleancache_filekey key, pgoff_t index, struct page *page)
 {
 	u32 ind = (u32) index;
 	struct tmem_oid oid = *(struct tmem_oid *)&key;
 	unsigned long pfn = page_to_pfn(page);
 
-	if (pool < 0)
-		return;
-	if (ind != index)
-		return;
+	 if (pool < 0)
+	 	return -1;
+	 if (ind != index)
+	  	return -1;
 	mb(); /* ensure page is quiescent; tmem may address it with an alias */
-	(void)xen_tmem_put_page((u32)pool, oid, ind, pfn);
+	
+	return xen_tmem_put_page((u32)pool, oid, ind, pfn);
 }
 
 static int tmem_cleancache_get_page(int pool, struct cleancache_filekey key,
 				    pgoff_t index, struct page *page)
 {
@@ -201,21 +201,20 @@
 		return 0;
 	else
 		return -1;
 }
 
-static void tmem_cleancache_flush_page(int pool, struct cleancache_filekey key,
-				       pgoff_t index)
+static int tmem_cleancache_flush_page(int pool, struct cleancache_filekey key, pgoff_t index)
 {
 	u32 ind = (u32) index;
 	struct tmem_oid oid = *(struct tmem_oid *)&key;
 
-	if (pool < 0)
-		return;
-	if (ind != index)
-		return;
-	(void)xen_tmem_flush_page((u32)pool, oid, ind);
+	 if (pool < 0)
+	  	return -1;
+	 if (ind != index)
+	  	return -1;
+	 return xen_tmem_flush_page((u32)pool, oid, ind);
 }
 
 static void tmem_cleancache_flush_inode(int pool, struct cleancache_filekey key)
 {
 	struct tmem_oid oid = *(struct tmem_oid *)&key;
@@ -405,10 +404,17 @@
 				err);
 		else
 			pr_info("cleancache enabled, RAM provided by "
 				"Xen Transcendent Memory\n");
 	}
+	else
+		pr_info("TMEM: Cleancache is not used tmem_enabled=%d cleancache=%d\n",tmem_enabled, cleancache);
+#endif
+#if (CONFIG_CLEANCACHE || CONFIG_FRONTSWAP)
+	pr_info("TMEM: is enabled during compile time\n");
+#else
+	pr_info("TMEM: is disabled in kconfig\n");
 #endif
 #ifdef CONFIG_XEN_SELFBALLOONING
 	/*
 	 * There is no point of driving pages to the swap system if they
 	 * aren't going anywhere in tmem universe.
diff -Nar -U 5 kernel_source/original/linux-4.1.3/fs/block_dev.c kernel_source/patched/linux-4.1.3/fs/block_dev.c
--- kernel_source/original/linux-4.1.3/fs/block_dev.c	2015-09-20 01:22:15.166585000 +0530
+++ kernel_source/patched/linux-4.1.3/fs/block_dev.c	2015-08-24 22:13:45.318845000 +0530
@@ -84,10 +84,14 @@
 	lru_add_drain_all();	/* make sure all lru add caches are flushed */
 	invalidate_mapping_pages(mapping, 0, -1);
 	/* 99% of the time, we don't need to flush the cleancache on the bdev.
 	 * But, for the strange corners, lets be cautious
 	 */
+	/*ABY MTP*/
+	//pr_info("MTP | IN FUNCTION: invalidate_bdev | INFO: calling cleancache_invalidate_inode for inode: %lu|\n", mapping->host->i_ino);
+	//printk(KERN_INFO"MTP | IN FUNCTION: invalidate_bdev | INFO: calling cleancache_invalidate_inode for inode: %lu|\n", mapping->host->i_ino);
+	/*ABY END MTP*/
 	cleancache_invalidate_inode(mapping);
 }
 EXPORT_SYMBOL(invalidate_bdev);
 
 int set_blocksize(struct block_device *bdev, int size)
diff -Nar -U 5 kernel_source/original/linux-4.1.3/fs/ext4/readpage.c kernel_source/patched/linux-4.1.3/fs/ext4/readpage.c
--- kernel_source/original/linux-4.1.3/fs/ext4/readpage.c	2015-09-20 01:22:16.698585000 +0530
+++ kernel_source/patched/linux-4.1.3/fs/ext4/readpage.c	2015-08-24 22:40:29.321618000 +0530
@@ -257,10 +257,15 @@
 				goto next_page;
 			}
 		} else if (fully_mapped) {
 			SetPageMappedToDisk(page);
 		}
+		/*ABY MTP*/
+		//pr_info("MTP | IN FUNCTION: ext4_mpage_readpages | INFO: calling cleancache_get_page for page: %lu |\n", page->index);
+		//printk(KERN_INFO"MTP | IN FUNCTION: ext4_mpage_readpages | INFO: calling cleancache_get_page for page: %lu |\n", page->index);
+		/*ABY END MTP*/
+
 		if (fully_mapped && blocks_per_page == 1 &&
 		    !PageUptodate(page) && cleancache_get_page(page) == 0) {
 			SetPageUptodate(page);
 			goto confused;
 		}
diff -Nar -U 5 kernel_source/original/linux-4.1.3/fs/ext4/super.c kernel_source/patched/linux-4.1.3/fs/ext4/super.c
--- kernel_source/original/linux-4.1.3/fs/ext4/super.c	2015-09-20 01:22:16.702585000 +0530
+++ kernel_source/patched/linux-4.1.3/fs/ext4/super.c	2015-08-26 12:55:23.393976000 +0530
@@ -1915,10 +1915,17 @@
 			sb->s_blocksize,
 			sbi->s_groups_count,
 			EXT4_BLOCKS_PER_GROUP(sb),
 			EXT4_INODES_PER_GROUP(sb),
 			sbi->s_mount_opt, sbi->s_mount_opt2);
+	
+	/*ABY MTP*/
+	/*
+	 * Uncomment this only if you are ready to fill your hdd space with log info!!
+	pr_info("MTP | IN FUNCTION: ext4_setup_super | INFO: Calling cleancache_init_fs |\n");
+	 */
+	/*END ABY MTP*/
 
 	cleancache_init_fs(sb);
 	return res;
 }
 
diff -Nar -U 5 kernel_source/original/linux-4.1.3/fs/inode.c kernel_source/patched/linux-4.1.3/fs/inode.c
--- kernel_source/original/linux-4.1.3/fs/inode.c	2015-09-20 01:22:12.514585000 +0530
+++ kernel_source/patched/linux-4.1.3/fs/inode.c	2015-08-26 13:03:42.033976000 +0530
@@ -17,10 +17,11 @@
 #include <linux/prefetch.h>
 #include <linux/buffer_head.h> /* for inode_has_buffers */
 #include <linux/ratelimit.h>
 #include <linux/list_lru.h>
 #include <trace/events/writeback.h>
+#include <linux/cleancache.h>
 #include "internal.h"
 
 /*
  * Inode locking rules:
  *
@@ -1433,10 +1434,23 @@
 	if (op->drop_inode)
 		drop = op->drop_inode(inode);
 	else
 		drop = generic_drop_inode(inode);
 
+	 /* If file is deleted, call cleancache flush inode. Not otherwise*/
+
+	if(drop && inode->i_mapping)
+	{
+		/* ABY MTP*/
+		/*
+		 * Uncomment this if you are ready to fill your hdd space with log info!!
+		pr_info("MTP | IN FUNCTION: iput_final | INFO: calling cleancache_invalidate_inode for inode: %lu|\n", inode->i_ino);
+		 */
+		/*END ABY MTP*/
+		cleancache_invalidate_inode(inode->i_mapping);
+	}
+
 	if (!drop && (sb->s_flags & MS_ACTIVE)) {
 		inode->i_state |= I_REFERENCED;
 		inode_add_lru(inode);
 		spin_unlock(&inode->i_lock);
 		return;
diff -Nar -U 5 kernel_source/original/linux-4.1.3/fs/mpage.c kernel_source/patched/linux-4.1.3/fs/mpage.c
--- kernel_source/original/linux-4.1.3/fs/mpage.c	2015-09-20 01:22:14.706585000 +0530
+++ kernel_source/patched/linux-4.1.3/fs/mpage.c	2015-08-24 22:47:49.961534000 +0530
@@ -254,10 +254,14 @@
 			goto out;
 		}
 	} else if (fully_mapped) {
 		SetPageMappedToDisk(page);
 	}
+	/*ABY MTP*/
+	//pr_info("MTP | IN FUNCTION: do_mpage_readpage | INFO: calling cleancache_get_page for page: %lu |\n", page->index);
+	//printk(KERN_INFO"MTP | IN FUNCTION: do_mpage_readpage | INFO: calling cleancache_get_page for page: %lu |\n", page->index);
+	/*ABY END MTP*/
 
 	if (fully_mapped && blocks_per_page == 1 && !PageUptodate(page) &&
 	    cleancache_get_page(page) == 0) {
 		SetPageUptodate(page);
 		goto confused;
diff -Nar -U 5 kernel_source/original/linux-4.1.3/fs/ocfs2/super.c kernel_source/patched/linux-4.1.3/fs/ocfs2/super.c
--- kernel_source/original/linux-4.1.3/fs/ocfs2/super.c	2015-09-20 01:22:15.962585000 +0530
+++ kernel_source/patched/linux-4.1.3/fs/ocfs2/super.c	2015-08-24 19:29:41.479413000 +0530
@@ -2333,10 +2333,14 @@
 	status = ocfs2_init_slot_info(osb);
 	if (status < 0) {
 		mlog_errno(status);
 		goto bail;
 	}
+	/*ABY MTP*/
+	pr_info("MTP | IN FUNCTION: ocfs2_initialize_super | INFO: Calling cleancache_init_shared_fs |\n");
+	printk(KERN_INFO"MTP | IN FUNCTION: ocfs2_initialize_super | INFO: Calling cleancache_init_shared_fs |\n");
+	/*ABY END MTP*/
 	cleancache_init_shared_fs(sb);
 
 bail:
 	return status;
 }
diff -Nar -U 5 kernel_source/original/linux-4.1.3/fs/super.c kernel_source/patched/linux-4.1.3/fs/super.c
--- kernel_source/original/linux-4.1.3/fs/super.c	2015-09-20 01:22:16.178585000 +0530
+++ kernel_source/patched/linux-4.1.3/fs/super.c	2015-08-24 20:27:50.695413000 +0530
@@ -279,10 +279,14 @@
  */
 void deactivate_locked_super(struct super_block *s)
 {
 	struct file_system_type *fs = s->s_type;
 	if (atomic_dec_and_test(&s->s_active)) {
+		/*ABY MTP*/
+		pr_info("MTP | IN FUNCTION: deactivate_locked_super | INFO: calling cleancache_invalidate_fs |\n");
+		printk(KERN_INFO"MTP | IN FUNCTION: deactivate_locked_super | INFO: calling cleancache_invalidate_fs |\n");
+		/*ABY END MTP*/
 		cleancache_invalidate_fs(s);
 		unregister_shrinker(&s->s_shrink);
 		fs->kill_sb(s);
 
 		/*
diff -Nar -U 5 kernel_source/original/linux-4.1.3/include/linux/cleancache.h kernel_source/patched/linux-4.1.3/include/linux/cleancache.h
--- kernel_source/original/linux-4.1.3/include/linux/cleancache.h	2015-09-20 01:22:21.082585000 +0530
+++ kernel_source/patched/linux-4.1.3/include/linux/cleancache.h	2015-08-26 12:32:46.793976000 +0530
@@ -28,13 +28,14 @@
 struct cleancache_ops {
 	int (*init_fs)(size_t);
 	int (*init_shared_fs)(char *uuid, size_t);
 	int (*get_page)(int, struct cleancache_filekey,
 			pgoff_t, struct page *);
-	void (*put_page)(int, struct cleancache_filekey,
+
+	int (*put_page)(int, struct cleancache_filekey,
 			pgoff_t, struct page *);
-	void (*invalidate_page)(int, struct cleancache_filekey, pgoff_t);
+ 	int (*invalidate_page)(int, struct cleancache_filekey, pgoff_t);
 	void (*invalidate_inode)(int, struct cleancache_filekey);
 	void (*invalidate_fs)(int);
 };
 
 extern int cleancache_register_ops(struct cleancache_ops *ops);
@@ -43,11 +44,11 @@
 extern int  __cleancache_get_page(struct page *);
 extern void __cleancache_put_page(struct page *);
 extern void __cleancache_invalidate_page(struct address_space *, struct page *);
 extern void __cleancache_invalidate_inode(struct address_space *);
 extern void __cleancache_invalidate_fs(struct super_block *);
-
+extern void __cleancache_invalidate_inode_range(struct address_space *mapping, loff_t start, loff_t end);
 #ifdef CONFIG_CLEANCACHE
 #define cleancache_enabled (1)
 static inline bool cleancache_fs_enabled(struct page *page)
 {
 	return page->mapping->host->i_sb->cleancache_poolid >= 0;
@@ -75,51 +76,112 @@
  * no measurable performance impact.
  */
 
 static inline void cleancache_init_fs(struct super_block *sb)
 {
+	/*ABY MTP*/
+	/*
+	 * Uncomment this only if you are ready to fill your hdd space with log info!!
+	pr_info("MTP | IN FUNCTION: cleancache_init_fs | INFO: cleancache_enabled: %d |\n", cleancache_enabled);
+	 */
+	/*ABY END MTP*/
+	
 	if (cleancache_enabled)
 		__cleancache_init_fs(sb);
 }
 
 static inline void cleancache_init_shared_fs(struct super_block *sb)
 {
+	/*ABY MTP*/
+	/*
+	 * Uncomment this only if you are ready to fill your hdd space with log info!!
+	pr_info("MTP | IN FUNCTION: cleancache_init_shared_fs | INFO: cleancache_enabled: %d|\n", cleancache_enabled);
+	 */
+	/*ABY END MTP*/
+	
 	if (cleancache_enabled)
 		__cleancache_init_shared_fs(sb);
 }
 
 static inline int cleancache_get_page(struct page *page)
 {
 	int ret = -1;
+	/*ABY MTP*/
+	/*
+	 * Uncomment this only if you are ready to fill your hdd space with log info!!
+	pr_info("MTP | IN FUNCTION: cleancache_get_page | INFO: cleancache_enabled: %d, cleancache_fs_enabled: %d |\n", cleancache_enabled, cleancache_fs_enabled(page));
+	 */
+	/*ABY END MTP*/
 
 	if (cleancache_enabled && cleancache_fs_enabled(page))
 		ret = __cleancache_get_page(page);
 	return ret;
 }
 
 static inline void cleancache_put_page(struct page *page)
 {
+	/*ABY MTP*/
+	/*
+	 * Uncomment this only if you are ready to fill your hdd space with log info!!
+	pr_info("MTP | IN FUNCTION: cleancache_put_page | INFO: cleancache_enabled: %d, cleancache_fs_enabled: %d |\n", cleancache_enabled, cleancache_fs_enabled(page));
+	 */
+	/*ABY END MTP*/
+
 	if (cleancache_enabled && cleancache_fs_enabled(page))
 		__cleancache_put_page(page);
 }
 
 static inline void cleancache_invalidate_page(struct address_space *mapping,
 					struct page *page)
 {
+	/*ABY MTP*/
+	/*
+	 * Uncomment this only if you are ready to fill your hdd space with log info!!
+	pr_info("MTP | IN FUNCTION: cleancache_invalidate_page | INFO: cleancache_enabled: %d, cleancache_fs_enabled_mapping(mapping): %d |\n", cleancache_enabled, cleancache_fs_enabled_mapping(mapping));
+	 */
+	/*ABY END MTP*/
+	
 	/* careful... page->mapping is NULL sometimes when this is called */
 	if (cleancache_enabled && cleancache_fs_enabled_mapping(mapping))
 		__cleancache_invalidate_page(mapping, page);
 }
 
 static inline void cleancache_invalidate_inode(struct address_space *mapping)
 {
+	/*ABY MTP*/
+	/*
+	 * Uncomment this only if you are ready to fill your hdd space with log info!!
+	pr_info("MTP | IN FUNCTION: cleancache_invalidate_inode | INFO: cleancache_enabled: %d, cleancache_fs_enabled_mapping(mapping): %d |\n", cleancache_enabled, cleancache_fs_enabled_mapping(mapping));
+	 */
+	/*ABY END MTP*/
+	
 	if (cleancache_enabled && cleancache_fs_enabled_mapping(mapping))
 		__cleancache_invalidate_inode(mapping);
 }
 
+static inline void cleancache_invalidate_inode_range(struct address_space *mapping, loff_t start, loff_t end)
+{
+	/*ABY MTP*/
+	/*
+	 * Uncomment this only if you are ready to fill your hdd space with log info!!
+	pr_info("MTP | IN FUNCTION: cleancache_invalidate_inode_range | INFO: cleancache_enabled: %d, cleancache_fs_enabled_mapping(mapping): %d |\n", cleancache_enabled, cleancache_fs_enabled_mapping(mapping));
+	 */
+	/*ABY END MTP*/
+
+	if (cleancache_enabled && cleancache_fs_enabled_mapping(mapping))
+		__cleancache_invalidate_inode_range(mapping, start, end);
+}
+
 static inline void cleancache_invalidate_fs(struct super_block *sb)
 {
+	/*ABY MTP*/
+	/*
+	 * Uncomment this only if you are ready to fill your hdd space with log info!!
+	pr_info("MTP | IN FUNCTION: cleancache_invalidate_fs | INFO: cleancache_enabled: %d|\n", cleancache_enabled);
+	 */
+	/*ABY END MTP*/
+
 	if (cleancache_enabled)
 		__cleancache_invalidate_fs(sb);
 }
 
 #endif /* _LINUX_CLEANCACHE_H */
diff -Nar -U 5 kernel_source/original/linux-4.1.3/Makefile kernel_source/patched/linux-4.1.3/Makefile
--- kernel_source/original/linux-4.1.3/Makefile	2015-09-20 01:21:39.282585000 +0530
+++ kernel_source/patched/linux-4.1.3/Makefile	2015-09-20 01:32:32.306585000 +0530
@@ -1,5 +1,6 @@
+
 VERSION = 4
 PATCHLEVEL = 1
 SUBLEVEL = 3
 EXTRAVERSION =
 NAME = Series 4800
@@ -1384,10 +1385,12 @@
 
 modules: $(module-dirs)
 	@$(kecho) '  Building modules, stage 2.';
 	$(Q)$(MAKE) -f $(srctree)/scripts/Makefile.modpost
 
+
+
 PHONY += modules_install
 modules_install: _emodinst_ _emodinst_post
 
 install-dir := $(if $(INSTALL_MOD_DIR),$(INSTALL_MOD_DIR),extra)
 PHONY += _emodinst_
diff -Nar -U 5 kernel_source/original/linux-4.1.3/mm/cleancache.c kernel_source/patched/linux-4.1.3/mm/cleancache.c
--- kernel_source/original/linux-4.1.3/mm/cleancache.c	2015-09-20 01:22:11.934585000 +0530
+++ kernel_source/patched/linux-4.1.3/mm/cleancache.c	2015-08-26 12:32:28.005976000 +0530
@@ -1,5 +1,6 @@
+
 /*
  * Cleancache frontend
  *
  * This code provides the generic "frontend" layer to call a matching
  * "backend" driver implementation of cleancache.  See
@@ -15,10 +16,13 @@
 #include <linux/fs.h>
 #include <linux/exportfs.h>
 #include <linux/mm.h>
 #include <linux/debugfs.h>
 #include <linux/cleancache.h>
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/pagemap.h>
 
 /*
  * cleancache_ops is set by cleancache_register_ops to contain the pointers
  * to the cleancache "backend" implementation functions.
  */
@@ -31,18 +35,41 @@
  */
 static u64 cleancache_succ_gets;
 static u64 cleancache_failed_gets;
 static u64 cleancache_puts;
 static u64 cleancache_invalidates;
+/*ABY MTP*/
+static u64 cleancache_failed_invalidates;
+static u64 cleancache_inode_invalidates;
+/*END ABY MTP*/
+static u64 cleancache_failed_puts;
 
 static void cleancache_register_ops_sb(struct super_block *sb, void *unused)
 {
+	/*ABY MTP*/
+	/*
+	 * Uncomment this only if you are ready to fill your hdd space with log info!!
+	pr_info("MTP | IN FUNCTION: cleancache_register_ops_sb | INFO: cleancache_register_ops_sb called |\n");
+	 */
+	/*ABY MTP END*/
 	switch (sb->cleancache_poolid) {
 	case CLEANCACHE_NO_BACKEND:
+		/*ABY MTP*/
+		/*
+		 * Uncomment this only if you are ready to fill your hdd space with log info!!
+		pr_info("MTP | IN FUNCTION: cleancache_register_ops_sb | INFO: CLEANCACHE_NO_BACKEND |\n");
+		 */
+		/*ABY MTP END*/
 		__cleancache_init_fs(sb);
 		break;
 	case CLEANCACHE_NO_BACKEND_SHARED:
+		/*ABY MTP*/
+		/*
+		 * Uncomment this only if you are ready to fill your hdd space with log info!!
+		pr_info("MTP | IN FUNCTION: cleancache_register_ops_sb | INFO: CLEANCACHE_NO_BACKEND_SHARED |\n");
+		 */
+		/*ABY MTP END*/
 		__cleancache_init_shared_fs(sb);
 		break;
 	}
 }
 
@@ -51,10 +78,16 @@
  */
 int cleancache_register_ops(struct cleancache_ops *ops)
 {
 	if (cmpxchg(&cleancache_ops, NULL, ops))
 		return -EBUSY;
+	/*ABY MTP*/
+	/*
+	 * Uncomment this only if you are ready to fill your hdd space with log info!!
+	pr_info("MTP | IN FUNCTION: cleancache_register_ops | INFO: cleancache_register_ops called |\n");
+	 */
+	/*ABY MTP END*/
 
 	/*
 	 * A cleancache backend can be built as a module and hence loaded after
 	 * a cleancache enabled filesystem has called cleancache_init_fs. To
 	 * handle such a scenario, here we call ->init_fs or ->init_shared_fs
@@ -112,13 +145,26 @@
 
 /* Called by a cleancache-enabled filesystem at time of mount */
 void __cleancache_init_fs(struct super_block *sb)
 {
 	int pool_id = CLEANCACHE_NO_BACKEND;
+	/*ABY MTP*/
+	/*
+	 * Uncomment this only if you are ready to fill your hdd space with log info!!
+	pr_info("MTP |IN FUNCTION: __cleancache_init_fs |  INFO: __cleancache_init_fs called |\n");
+	 */
+	/*ABY MTP END*/
 
 	if (cleancache_ops) {
 		pool_id = cleancache_ops->init_fs(PAGE_SIZE);
+		/*ABY MTP*/
+		/*
+		 * Uncomment this only if you are ready to fill your hdd space with log info!!
+		pr_info("MTP |IN FUNCTION: __cleancache_init_fs |  INFO: pool_id: %d|\n", pool_id);
+		 */
+		/*ABY MTP END*/
+		pool_id = cleancache_ops->init_fs(PAGE_SIZE);
 		if (pool_id < 0)
 			pool_id = CLEANCACHE_NO_POOL;
 	}
 	sb->cleancache_poolid = pool_id;
 }
@@ -126,13 +172,25 @@
 
 /* Called by a cleancache-enabled clustered filesystem at time of mount */
 void __cleancache_init_shared_fs(struct super_block *sb)
 {
 	int pool_id = CLEANCACHE_NO_BACKEND_SHARED;
+	/*ABY MTP*/
+	/*
+	 * Uncomment this only if you are ready to fill your hdd space with log info!!
+	pr_info("MTP |IN FUNCTION: __cleancache_init_shared_fs |  INFO: __cleancache_init_shared_fs called |\n");
+	 */
+	/*ABY MTP END*/
 
 	if (cleancache_ops) {
 		pool_id = cleancache_ops->init_shared_fs(sb->s_uuid, PAGE_SIZE);
+		/*ABY MTP*/
+		/*
+		 * Uncomment this only if you are ready to fill your hdd space with log info!!
+		pr_info("MTP |IN FUNCTION: __cleancache_init_shared_fs |  INFO: pool_id: %d|\n", pool_id);
+		 */
+		/*ABY MTP END*/
 		if (pool_id < 0)
 			pool_id = CLEANCACHE_NO_POOL;
 	}
 	sb->cleancache_poolid = pool_id;
 }
@@ -148,19 +206,31 @@
 	int (*fhfn)(struct inode *, __u32 *fh, int *, struct inode *);
 	int len = 0, maxlen = CLEANCACHE_KEY_MAX;
 	struct super_block *sb = inode->i_sb;
 
 	key->u.ino = inode->i_ino;
+	/*ABY MTP*/
+	/*
+	 * Uncomment this only if you are ready to fill your hdd space with log info!!
+	pr_info("MTP |IN FUNCTION: cleancache_get_key |  INFO: inode: %lu |\n", inode->i_ino);
+	 */
+	/*ABY MTP END*/
 	if (sb->s_export_op != NULL) {
 		fhfn = sb->s_export_op->encode_fh;
 		if  (fhfn) {
 			len = (*fhfn)(inode, &key->u.fh[0], &maxlen, NULL);
 			if (len <= FILEID_ROOT || len == FILEID_INVALID)
 				return -1;
 			if (maxlen > CLEANCACHE_KEY_MAX)
 				return -1;
 		}
+		/*ABY MTP*/
+		/*
+		 * Uncomment this only if you are ready to fill your hdd space with log info!!
+		pr_info("MTP |IN FUNCTION: cleancache_get_key |  INFO: file handle[0]: %du |\n", key->u.fh[0]);
+		 */
+		/*ABY MTP END*/
 	}
 	return 0;
 }
 
 /*
@@ -196,10 +266,16 @@
 	ret = cleancache_ops->get_page(pool_id, key, page->index, page);
 	if (ret == 0)
 		cleancache_succ_gets++;
 	else
 		cleancache_failed_gets++;
+	/*ABY MTP*/
+	/*
+	 * Uncomment this only if you are ready to fill your hdd space with log info!!
+	pr_info("MTP |IN FUNCTION: __cleancache_get_page |  INFO: page_index: %lu |\n", page->index);
+	 */
+	/*ABY MTP END*/
 out:
 	return ret;
 }
 EXPORT_SYMBOL(__cleancache_get_page);
 
@@ -223,14 +299,21 @@
 		return;
 	}
 
 	VM_BUG_ON_PAGE(!PageLocked(page), page);
 	pool_id = page->mapping->host->i_sb->cleancache_poolid;
-	if (pool_id >= 0 &&
-		cleancache_get_key(page->mapping->host, &key) >= 0) {
-		cleancache_ops->put_page(pool_id, key, page->index, page);
+	
+	if (pool_id >= 0 && cleancache_get_key(page->mapping->host, &key) >= 0) {
 		cleancache_puts++;
+		if(cleancache_ops->put_page(pool_id, key, page->index, page))
+			cleancache_failed_puts++;
+		/*ABY MTP*/
+		/*
+		 * Uncomment this only if you are ready to fill your hdd space with log info!!
+		pr_info("MTP |IN FUNCTION: __cleancache_put_page |  INFO: page_index: %lu |\n", page->index);
+		 */
+		/*ABY MTP END*/
 	}
 }
 EXPORT_SYMBOL(__cleancache_put_page);
 
 /*
@@ -252,13 +335,19 @@
 		return;
 
 	if (pool_id >= 0) {
 		VM_BUG_ON_PAGE(!PageLocked(page), page);
 		if (cleancache_get_key(mapping->host, &key) >= 0) {
-			cleancache_ops->invalidate_page(pool_id,
-					key, page->index);
+			if(cleancache_ops->invalidate_page(pool_id, key, page->index))
+				cleancache_failed_invalidates++;
 			cleancache_invalidates++;
+			/*ABY MTP*/
+			/*
+			 * Uncomment this only if you are ready to fill your hdd space with log info!!
+			pr_info("MTP |IN FUNCTION: __cleancache_invalidate_page |  INFO: page_index: %lu |\n", page->index);
+			 */
+			/*ABY MTP END*/
 		}
 	}
 }
 EXPORT_SYMBOL(__cleancache_invalidate_page);
 
@@ -278,14 +367,47 @@
 
 	if (!cleancache_ops)
 		return;
 
 	if (pool_id >= 0 && cleancache_get_key(mapping->host, &key) >= 0)
-		cleancache_ops->invalidate_inode(pool_id, key);
+	{
+	  	cleancache_ops->invalidate_inode(pool_id, key);
+		cleancache_inode_invalidates++;
+		/*ABY MTP*/
+		/*
+		 * Uncomment this only if you are ready to fill your hdd space with log info!!
+		pr_info("MTP |IN FUNCTION: __cleancache_invalidate_inode |  INFO: inode: %lu |\n", key.u.ino);
+		 */
+		/*ABY MTP END*/
+	}
 }
 EXPORT_SYMBOL(__cleancache_invalidate_inode);
 
+/*Invalidate inode between start and end*/
+void __cleancache_invalidate_inode_range(struct address_space *mapping, loff_t start, loff_t end)
+{
+	unsigned long count = 0;
+	int pool_id = mapping->host->i_sb->cleancache_poolid;
+	struct cleancache_filekey key = { .u.key = {0} };
+		pgoff_t index = (start + PAGE_CACHE_SIZE -1) >> PAGE_CACHE_SHIFT;
+		pgoff_t fend = (end >> PAGE_CACHE_SHIFT);
+		if(pool_id < 0)
+			return;
+		if(cleancache_get_key(mapping->host, &key) < 0)
+			return;
+
+		while(index <= fend){
+			if(cleancache_ops->invalidate_page(pool_id, key, index))
+				cleancache_failed_invalidates++;
+			cleancache_invalidates++;
+			cond_resched();
+			index++;
+			count++;
+		}
+		return;
+}
+
 /*
  * Called by any cleancache-enabled filesystem at time of unmount;
  * note that pool_id is surrendered and may be returned by a subsequent
  * cleancache_init_fs or cleancache_init_shared_fs.
  */
@@ -296,10 +418,16 @@
 	pool_id = sb->cleancache_poolid;
 	sb->cleancache_poolid = CLEANCACHE_NO_POOL;
 
 	if (cleancache_ops && pool_id >= 0)
 		cleancache_ops->invalidate_fs(pool_id);
+	/*ABY MTP*/
+	/*
+	 * Uncomment this only if you are ready to fill your hdd space with log info!!
+	pr_info("MTP |IN FUNCTION: __cleancache_invalidate_fs |  INFO: pool_id: %d |\n", pool_id);
+	 */
+	/*ABY MTP END*/
 }
 EXPORT_SYMBOL(__cleancache_invalidate_fs);
 
 static int __init init_cleancache(void)
 {
@@ -311,9 +439,17 @@
 	debugfs_create_u64("failed_gets", S_IRUGO,
 				root, &cleancache_failed_gets);
 	debugfs_create_u64("puts", S_IRUGO, root, &cleancache_puts);
 	debugfs_create_u64("invalidates", S_IRUGO,
 				root, &cleancache_invalidates);
+	/*ABY MTP*/
+	debugfs_create_u64("inode_invalidates", S_IRUGO,
+				root, &cleancache_inode_invalidates);
+	debugfs_create_u64("failed_invalidates", S_IRUGO,
+				root, &cleancache_failed_invalidates);
+	/*ABY MTP END*/
+	debugfs_create_u64("failed_puts", S_IRUGO,
+				root, &cleancache_failed_puts);
 #endif
 	return 0;
 }
 module_init(init_cleancache)
diff -Nar -U 5 kernel_source/original/linux-4.1.3/mm/filemap.c kernel_source/patched/linux-4.1.3/mm/filemap.c
--- kernel_source/original/linux-4.1.3/mm/filemap.c	2015-09-20 01:22:11.870585000 +0530
+++ kernel_source/patched/linux-4.1.3/mm/filemap.c	2015-08-24 22:54:31.681364000 +0530
@@ -184,14 +184,29 @@
 	/*
 	 * if we're uptodate, flush out into the cleancache, otherwise
 	 * invalidate any existing cleancache entries.  We can't leave
 	 * stale data around in the cleancache once our page is gone
 	 */
-	if (PageUptodate(page) && PageMappedToDisk(page))
+
+	/*ABY MTP*/
+	/*
+	 *if (PageUptodate(page) && PageMappedToDisk(page))
+	 *	cleancache_put_page(page);
+	 *else
+	 *	cleancache_invalidate_page(mapping, page);
+	 */
+	if (PageUptodate(page) && PageMappedToDisk(page)) {
+		//pr_info("MTP | IN FUNCTION: __delete_from_page_cache | INFO: calling cleancache_put_page for page: %lu |\n", page->index);
+		//printk(KERN_INFO"MTP | IN FUNCTION: __delete_from_page_cache | INFO: calling cleancache_put_page for page: %lu |\n", page->index);
 		cleancache_put_page(page);
-	else
+	}
+	else {
+		//pr_info("MTP | IN FUNCTION: __delete_from_page_cache | INFO: calling cleancache_invalidate_page for page: %lu |\n", page->index);
+		//printk(KERN_INFO"MTP | IN FUNCTION: __delete_from_page_cache | INFO: calling cleancache_invalidate_page for page: %lu |\n", page->index);
 		cleancache_invalidate_page(mapping, page);
+	}
+	/*ABY END MTP*/
 
 	page_cache_tree_delete(mapping, page, shadow);
 
 	page->mapping = NULL;
 	/* Leave page->index set: truncation lookup relies upon it */
@@ -1063,10 +1078,17 @@
  * If there is a page cache page, it is returned with an increased refcount.
  */
 struct page *pagecache_get_page(struct address_space *mapping, pgoff_t offset,
 	int fgp_flags, gfp_t gfp_mask)
 {
+	/*ABY MTP*/
+	/*
+	 * Safety cleancache flush call before the page in pagecache is written to is not, as it is
+	 * there in the tmem_guest patch is not incorporated here.
+	 *
+	 */
+	/*END ABY MTP*/
 	struct page *page;
 
 repeat:
 	page = find_get_entry(mapping, offset);
 	if (radix_tree_exceptional_entry(page))
diff -Nar -U 5 kernel_source/original/linux-4.1.3/mm/Kconfig kernel_source/patched/linux-4.1.3/mm/Kconfig
--- kernel_source/original/linux-4.1.3/mm/Kconfig	2015-09-20 01:22:11.814585000 +0530
+++ kernel_source/patched/linux-4.1.3/mm/Kconfig	2015-08-05 18:23:33.200282000 +0530
@@ -453,11 +453,11 @@
 	bool
 	default y
 
 config CLEANCACHE
 	bool "Enable cleancache driver to cache clean pages if tmem is present"
-	default n
+	default y
 	help
 	  Cleancache can be thought of as a page-granularity victim cache
 	  for clean pages that the kernel's pageframe replacement algorithm
 	  (PFRA) would like to keep around, but can't since there isn't enough
 	  memory.  So when the PFRA "evicts" a page, it first attempts to use
diff -Nar -U 5 kernel_source/original/linux-4.1.3/mm/truncate.c kernel_source/patched/linux-4.1.3/mm/truncate.c
--- kernel_source/original/linux-4.1.3/mm/truncate.c	2015-09-20 01:22:11.850585000 +0530
+++ kernel_source/patched/linux-4.1.3/mm/truncate.c	2015-08-26 13:19:29.369976000 +0530
@@ -215,11 +215,12 @@
  *
  * Note that since ->invalidatepage() accepts range to invalidate
  * truncate_inode_pages_range is able to handle cases where lend + 1 is not
  * page aligned properly.
  */
-void truncate_inode_pages_range(struct address_space *mapping,
+void truncate_inode_pages_range(struct 
+		address_space *mapping,
 				loff_t lstart, loff_t lend)
 {
 	pgoff_t		start;		/* inclusive */
 	pgoff_t		end;		/* exclusive */
 	unsigned int	partial_start;	/* inclusive */
@@ -227,11 +228,18 @@
 	struct pagevec	pvec;
 	pgoff_t		indices[PAGEVEC_SIZE];
 	pgoff_t		index;
 	int		i;
 
-	cleancache_invalidate_inode(mapping);
+	/*ABY MTP*/ 
+	/*
+	 * Uncomment this if you are ready to fill your hdd space with log info!!
+	pr_info("MTP | IN FUNCTION: truncate_inode_pages_range | INFO: calling cleancache_invalidate_inode for inode: %lu|\n", mapping->host->i_ino);
+	 */
+	/*ABY END MTP*/
+	//cleancache_invalidate_inode(mapping);
+	
 	if (mapping->nrpages == 0 && mapping->nrshadows == 0)
 		return;
 
 	/* Offsets within partial pages */
 	partial_start = lstart & (PAGE_CACHE_SIZE - 1);
@@ -297,10 +305,18 @@
 				top = partial_end;
 				partial_end = 0;
 			}
 			wait_on_page_writeback(page);
 			zero_user_segment(page, partial_start, top);
+
+			/*ABY MTP*/
+			/*
+			 * Uncomment this if you are ready to fill your hdd space with log info!!
+			pr_info("MTP | IN FUNCTION: truncate_inode_pages_range | INFO: calling cleancache_invalidate_page for page: %lu |\n", page->index);
+			 */
+			/*ABY END MTP*/
+
 			cleancache_invalidate_page(mapping, page);
 			if (page_has_private(page))
 				do_invalidatepage(page, partial_start,
 						  top - partial_start);
 			unlock_page(page);
@@ -310,10 +326,17 @@
 	if (partial_end) {
 		struct page *page = find_lock_page(mapping, end);
 		if (page) {
 			wait_on_page_writeback(page);
 			zero_user_segment(page, 0, partial_end);
+
+			/*ABY MTP*/
+			/*
+			 * Uncomment this if you are ready to fill your hdd space with log info!!
+			pr_info("MTP | IN FUNCTION: truncate_inode_pages_range | INFO: calling cleancache_invalidate_page for page: %lu |\n", page->index);
+			 */
+			/*ABY END MTP*/
 			cleancache_invalidate_page(mapping, page);
 			if (page_has_private(page))
 				do_invalidatepage(page, 0,
 						  partial_end);
 			unlock_page(page);
@@ -369,11 +392,19 @@
 		}
 		pagevec_remove_exceptionals(&pvec);
 		pagevec_release(&pvec);
 		index++;
 	}
-	cleancache_invalidate_inode(mapping);
+
+	/*ABY MTP*/
+	/*
+	 * Uncomment this if you are ready to fill your hdd space with log info!!
+	pr_info("MTP | IN FUNCTION: truncate_inode_pages_range | INFO: calling cleancache_invalidate_inode for inode: %lu|\n", mapping->host->i_ino);
+	 */
+	/*ABY END MTP*/
+	
+   	//cleancache_invalidate_inode(mapping);
 }
 EXPORT_SYMBOL(truncate_inode_pages_range);
 
 /**
  * truncate_inode_pages - truncate *all* the pages from an offset
@@ -564,10 +595,17 @@
 	pgoff_t index;
 	int i;
 	int ret = 0;
 	int ret2 = 0;
 	int did_range_unmap = 0;
+	
+	/*ABY MTP*/
+	/*
+	 * Uncomment this if you are ready to fill your hdd space with log info!!
+	pr_info("MTP | IN FUNCTION: truncate_inode_pages2_range | INFO: calling cleancache_invalidate_inode for inode: %lu|\n", mapping->host->i_ino);
+	 */
+	/*ABY END MTP*/
 
 	cleancache_invalidate_inode(mapping);
 	pagevec_init(&pvec, 0);
 	index = start;
 	while (index <= end && pagevec_lookup_entries(&pvec, mapping, index,
@@ -626,10 +664,16 @@
 		pagevec_remove_exceptionals(&pvec);
 		pagevec_release(&pvec);
 		cond_resched();
 		index++;
 	}
+	/*ABY MTP*/
+	/*
+	 * Uncomment this if you are ready to fill your hdd space with log info!!
+	pr_info("MTP | IN FUNCTION: truncate_inode_pages2_range | INFO: calling cleancache_invalidate_inode for inode: %lu|\n", mapping->host->i_ino);
+	 */
+	/*ABY END MTP*/
 	cleancache_invalidate_inode(mapping);
 	return ret;
 }
 EXPORT_SYMBOL_GPL(invalidate_inode_pages2_range);
 
@@ -666,10 +710,21 @@
 void truncate_pagecache(struct inode *inode, loff_t newsize)
 {
 	struct address_space *mapping = inode->i_mapping;
 	loff_t holebegin = round_up(newsize, PAGE_SIZE);
 
+	/*Flush cleancache of the pages that are not in page cache
+	 * Repeated flush can happen, but better than full file flush.
+	 * Hence, moved from truncate_inode_page_range 
+	 */
+	
+	loff_t oldsize = inode->i_size;
+
+	if(newsize < oldsize){
+		cleancache_invalidate_inode_range(mapping, newsize, oldsize);
+	}
+	
 	/*
 	 * unmap_mapping_range is called twice, first simply for
 	 * efficiency so that truncate_inode_pages does fewer
 	 * single-page unmaps.  However after this first call, and
 	 * before truncate_inode_pages finishes, it is possible for
