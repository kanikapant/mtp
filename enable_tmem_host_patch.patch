diff -Nr -U 5 linux-4.1.3/arch/x86/kvm/Kconfig patched_kernels/linux-4.1.3/arch/x86/kvm/Kconfig
--- linux-4.1.3/arch/x86/kvm/Kconfig	2015-07-21 22:40:33.000000000 +0530
+++ patched_kernels/linux-4.1.3/arch/x86/kvm/Kconfig	2015-09-23 14:01:09.236219305 +0530
@@ -99,6 +99,10 @@
 # OK, it's a little counter-intuitive to do this, but it puts it neatly under
 # the virtualization menu.
 source drivers/vhost/Kconfig
 source drivers/lguest/Kconfig
 
+#ABY MTP
+source drivers/staging/kvm_tmem_backend/Kconfig
+#END ABY MTP
+
 endif # VIRTUALIZATION
diff -Nr -U 5 linux-4.1.3/arch/x86/kvm/Makefile patched_kernels/linux-4.1.3/arch/x86/kvm/Makefile
--- linux-4.1.3/arch/x86/kvm/Makefile	2015-07-21 22:40:33.000000000 +0530
+++ patched_kernels/linux-4.1.3/arch/x86/kvm/Makefile	2015-09-23 18:48:05.997563463 +0530
@@ -4,19 +4,25 @@
 CFLAGS_x86.o := -I.
 CFLAGS_svm.o := -I.
 CFLAGS_vmx.o := -I.
 
 KVM := ../../../virt/kvm
+#KVM_TMEM_BKND := ../../../drivers/staging/kvm_tmem_backend
 
 kvm-y			+= $(KVM)/kvm_main.o $(KVM)/coalesced_mmio.o \
 				$(KVM)/eventfd.o $(KVM)/irqchip.o $(KVM)/vfio.o
 kvm-$(CONFIG_KVM_ASYNC_PF)	+= $(KVM)/async_pf.o
 
 kvm-y			+= x86.o mmu.o emulate.o i8259.o irq.o lapic.o \
 			   i8254.o ioapic.o irq_comm.o cpuid.o pmu.o
 kvm-$(CONFIG_KVM_DEVICE_ASSIGNMENT)	+= assigned-dev.o iommu.o
+#ABY MTP
+#KVM(Host) tmem backend directory
+kvm-$(CONFIG_KVM_TMEM_BKND) += ../../../drivers/staging/kvm_tmem_backend/kvm_tmem.o
+#END ABY MTP
 kvm-intel-y		+= vmx.o
 kvm-amd-y		+= svm.o
 
 obj-$(CONFIG_KVM)	+= kvm.o
 obj-$(CONFIG_KVM_INTEL)	+= kvm-intel.o
 obj-$(CONFIG_KVM_AMD)	+= kvm-amd.o
+
diff -Nr -U 5 linux-4.1.3/arch/x86/kvm/x86.c patched_kernels/linux-4.1.3/arch/x86/kvm/x86.c
--- linux-4.1.3/arch/x86/kvm/x86.c	2015-07-21 22:40:33.000000000 +0530
+++ patched_kernels/linux-4.1.3/arch/x86/kvm/x86.c	2015-09-23 20:29:10.781667857 +0530
@@ -48,11 +48,13 @@
 #include <linux/hash.h>
 #include <linux/pci.h>
 #include <linux/timekeeper_internal.h>
 #include <linux/pvclock_gtod.h>
 #include <trace/events/kvm.h>
-
+/*ABY MTP*/
+/* #include <linux/tmem.h> */
+/*ABY END MTP*/
 #define CREATE_TRACE_POINTS
 #include "trace.h"
 
 #include <asm/debugreg.h>
 #include <asm/msr.h>
@@ -5955,14 +5957,22 @@
 
 	lapic_irq.delivery_mode = APIC_DM_REMRD;
 	kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
 }
 
+/*ABY MTP*/
+#define KVM_TMEM_HCALL 10
+extern int kvm_host_tmem_handler(struct kvm_vcpu* vcpu, gpa_t, unsigned long* ret); 
+/*END ABY MTP*/
+
 int kvm_emulate_hypercall(struct kvm_vcpu *vcpu)
 {
 	unsigned long nr, a0, a1, a2, a3, ret;
 	int op_64_bit, r = 1;
+	/*ABY MTP*/
+	/* struct kvm_tmem_op op; */
+	/*END ABY MTP*/
 
 	kvm_x86_ops->skip_emulated_instruction(vcpu);
 
 	if (kvm_hv_hypercall_enabled(vcpu->kvm))
 		return kvm_hv_hypercall(vcpu);
@@ -5995,10 +6005,33 @@
 		break;
 	case KVM_HC_KICK_CPU:
 		kvm_pv_kick_cpu_op(vcpu->kvm, a0, a1);
 		ret = 0;
 		break;
+
+	/*ABY MTP*/
+	case KVM_TMEM_HCALL:
+		
+		/* pr_info("MTP | IN FUNCTION: kvm_emulate_hypercall | INFO: Inside KVM_TMEM_HCALL handler |\n"); */ 
+
+		/* r = kvm_read_guest(vcpu->kvm, a0, &op, sizeof(op)); */
+		
+		/* pr_info("MTP | IN FUNCTION: kvm_emulate_hypercall | INFO: Read  KVM_TMEM_HCALL using kvm_read_guest |\n"); */ 
+		/*
+		if(r < 0){
+			ret = ret - 1000;
+			break;
+		}
+		*/
+		
+		/* r = kvm_host_tmem_op(vcpu, a0, &ret); */
+		// r = kvm_host_tmem_op(/1* vcpu, struct tmem_oid *oid,*/&op, &ret); 
+		/* r = kvm_host_tmem_handler(vcpu, &op, &ret); */
+		r = kvm_host_tmem_handler(vcpu, a0, &ret); 
+		break;
+	/*ABY MTP END*/
+
 	default:
 		ret = -KVM_ENOSYS;
 		break;
 	}
 out:
diff -Nr -U 5 linux-4.1.3/drivers/staging/Kconfig patched_kernels/linux-4.1.3/drivers/staging/Kconfig
--- linux-4.1.3/drivers/staging/Kconfig	2015-07-21 22:40:33.000000000 +0530
+++ patched_kernels/linux-4.1.3/drivers/staging/Kconfig	2015-09-23 17:23:00.308465204 +0530
@@ -1,8 +1,8 @@
 menuconfig STAGING
 	bool "Staging drivers"
-	default n
+	default y
 	---help---
 	  This option allows you to select a number of drivers that are
 	  not of the "normal" Linux kernel quality level.  These drivers
 	  are placed here in order to get a wider audience to make use of
 	  them.  Please note that these drivers are under heavy
@@ -110,6 +110,10 @@
 
 source "drivers/staging/i2o/Kconfig"
 
 source "drivers/staging/fsl-mc/Kconfig"
 
+#ABY MTP#
+#source "drivers/staging/kvm_tmem_backend/Kconfig"
+#END ABY MTP#
+
 endif # STAGING
diff -Nr -U 5 linux-4.1.3/drivers/staging/kvm_tmem_backend/Kconfig patched_kernels/linux-4.1.3/drivers/staging/kvm_tmem_backend/Kconfig
--- linux-4.1.3/drivers/staging/kvm_tmem_backend/Kconfig	1970-01-01 05:30:00.000000000 +0530
+++ patched_kernels/linux-4.1.3/drivers/staging/kvm_tmem_backend/Kconfig	2015-09-23 18:27:02.736957914 +0530
@@ -0,0 +1,26 @@
+#ABY MTP#
+config KVM_TMEM_BKND
+	tristate "KVM tmem backend"
+	default y
+	---help---
+	  This option is selected by the drivers that 
+	  actually provides KVM host tmem backend support.
+
+menu "KVM tmem backend drivers"
+
+config KVM_TMEM_HOST
+	tristate "KVM host tmem backend support"
+	default y
+	---help---
+	This option enables KVM host transcendant memory 
+	backend to intercept KVM guest operations.
+
+#config KTB_MAIN
+#	tristate "KVM tmem backend main"
+#	default m
+#	---help---
+#	This option provides the actual KVM host tmem 
+#	functionality. 
+
+endmenu
+#END ABY MTP#
diff -Nr -U 5 linux-4.1.3/drivers/staging/kvm_tmem_backend/kvm_tmem.c patched_kernels/linux-4.1.3/drivers/staging/kvm_tmem_backend/kvm_tmem.c
--- linux-4.1.3/drivers/staging/kvm_tmem_backend/kvm_tmem.c	1970-01-01 05:30:00.000000000 +0530
+++ patched_kernels/linux-4.1.3/drivers/staging/kvm_tmem_backend/kvm_tmem.c	2015-10-07 13:12:27.382120233 +0530
@@ -0,0 +1,173 @@
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/types.h>
+#include <linux/kvm_types.h>
+#include <linux/kvm_host.h>
+#include <linux/kvm_para.h>
+#include <linux/tmem.h>
+/*ABY MTP*/
+/*kvm host tmem backend operations to support the guest tmem operations starts here*/
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Aby Sam Ross");
+
+char* kvmtmembknd  = "no";
+char* kvmtmemdedup = "no";
+char* nocleancache = "no";
+
+module_param(kvmtmembknd, charp, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP);
+MODULE_PARM_DESC(kvmtmembknd, "Parameter to enable kvm_tmem_bknd");
+
+module_param(kvmtmemdedup, charp, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP);
+MODULE_PARM_DESC(kvmtmemdedup, "Parameter to enable kvm_tmem_dedup");
+
+module_param(nocleancache, charp, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP);
+MODULE_PARM_DESC(nocleancache, "Parameter to disable cleancache");
+
+int kvm_tmem_bknd_enabled __read_mostly;
+EXPORT_SYMBOL(kvm_tmem_bknd_enabled);
+
+int kvm_tmem_dedup_enabled __read_mostly;
+EXPORT_SYMBOL(kvm_tmem_dedup_enabled);
+
+int use_cleancache = 1;
+EXPORT_SYMBOL(use_cleancache);
+
+static struct kvm_host_tmem_ops kvm_host_tmem_ops; 		
+
+void kvm_host_tmem_register_ops(struct kvm_host_tmem_ops *ops)
+{
+	kvm_host_tmem_ops = *ops;
+}
+EXPORT_SYMBOL(kvm_host_tmem_register_ops);
+
+/* int kvm_host_tmem_op (struct kvm_vcpu *vcpu, gpa_t addr, unsigned long *ret) */
+//int kvm_host_tmem_op (/*struct kvm_vcpu *vcpu, struct tmem_oid *oid,*/struct kvm_tmem_op *op, unsigned long *ret)
+int kvm_host_tmem_handler(struct kvm_vcpu *vcpu, gpa_t addr, unsigned long *ret)
+{
+	/* struct kvm_tmem_op op = *kvm_tmem_op; */
+	struct kvm_tmem_op op;
+	uint64_t pfn;
+	struct page *page;
+	struct tmem_oid oid;
+	int r;
+
+	r = kvm_read_guest(vcpu->kvm, addr, &op, sizeof(op));
+	if (r < 0)
+		return r;
+
+	switch (op.cmd) {
+		case TMEM_NEW_POOL:
+
+			pr_info("MTP | CALL: kvm_host_tmem_op | RESPONSE TO: tmem_init_fs\n");
+			if(kvm_host_tmem_ops.kvm_host_new_pool != NULL)
+				*ret = (*kvm_host_tmem_ops.kvm_host_new_pool)(op.u.new.cli_id, op.u.new.uuid[0], op.u.new.uuid[1], op.u.new.flags);
+			else
+			{
+				pr_info("MTP | No definiton for TMEM_NEW_POOL\n");
+				*ret = -1;
+			}	
+		       	break;
+
+		case TMEM_DESTROY_POOL:
+
+			pr_info("MTP | CALL: kvm_host_tmem_op | RESPONSE TO: tmem_flush_fs\n");
+			if(kvm_host_tmem_ops.kvm_host_destroy_pool != NULL)
+				*ret = (*kvm_host_tmem_ops.kvm_host_destroy_pool)(op.u.gen.cli_id, op.pool_id);
+			else
+			{
+				pr_info("MTP | No definition for TMEM_DESTROY_POOL\n");
+				*ret = -1;
+			}
+		       	break;
+
+		case TMEM_NEW_PAGE:
+			//pr_info("MTP | CALL: kvm_host_tmem_op | RESPONSE TO: tmem_new_page\n");
+
+			/* if(kvm_host_handle_new_pool) */
+			/* 	kvm_host_serv_new_page_request(); */
+		       	break;
+			
+		case TMEM_PUT_PAGE:
+			/*
+			pr_info("MTP | CALL: kvm_host_tmem_op | RESPONSE TO: tmem_put_page\n"); 
+			*/	
+			if(kvm_host_tmem_ops.kvm_host_put_page != NULL) 
+			{
+				pfn = gfn_to_pfn(vcpu->kvm, op.u.gen.gfn);
+				page = pfn_to_page(pfn);
+
+				oid.oid[0] = op.u.gen.oid[0];	
+				oid.oid[1] = op.u.gen.oid[1];	
+				oid.oid[2] = op.u.gen.oid[2];	
+				VM_BUG_ON(!PageLocked(page));				
+			 	*ret = (*kvm_host_tmem_ops.kvm_host_put_page)(op.u.gen.cli_id, op.pool_id, &oid, op.u.gen.index, page);
+			}
+			else
+				*ret = -1;
+		       	break;
+
+		case TMEM_GET_PAGE:
+			*ret = -1;
+			//pr_info("MTP | CALL: kvm_host_tmem_op | RESPONSE TO: tmem_get_page\n");
+			
+			/* if(kvm_host_handle_new_pool) */
+			/* 	kvm_host_serv_get_page_request(); */
+		       	break;
+
+		case TMEM_FLUSH_PAGE:
+			*ret = -1;
+			/* pr_info("MTP | CALL: kvm_host_tmem_op | RESPONSE TO: tmem_flush_page\n"); */
+
+			/* if(kvm_host_handle_new_pool) */
+			/* 	kvm_host_serv_flush_page_request(); */
+		       	break;
+
+		case TMEM_FLUSH_OBJECT:
+			*ret = -1;
+			/* pr_info("MTP | CALL: kvm_host_tmem_op | RESPONSE TO: tmem_flush_inode\n"); */
+
+			/* if(kvm_host_handle_new_pool) */
+			/* 	kvm_host_serv_flush_object_request(); */
+		       	break;
+
+		default:
+			pr_info("MTP | CALL: kvm_host_tmem_op | RESPONSE TO: default case\n");
+			*ret = -1;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(kvm_host_tmem_handler);
+
+static int __init kvm_host_tmem_bknd_init(void)
+{
+	/*Check if kvm tmem backend enabled*/
+	if(strcmp(kvmtmembknd,"yes") == 0)
+		kvm_tmem_bknd_enabled = 1;
+	else if(strcmp(kvmtmembknd,"no") == 0)
+		kvm_tmem_bknd_enabled = 0;
+	
+	/*Check if kvm tmem de-duplication enabled*/
+	if(strcmp(kvmtmemdedup,"yes") == 0)
+		kvm_tmem_dedup_enabled = 1;
+	else if(strcmp(kvmtmemdedup,"no") == 0)
+		kvm_tmem_dedup_enabled = 0;
+
+	/*Check if cleancache enabled*/
+	if(strcmp(nocleancache,"no") == 0)
+		use_cleancache = 1;
+		
+	return 0;
+}
+
+void __exit kvm_host_tmem_bknd_exit(void)
+{
+	kvmtmembknd = "no";
+		kvm_tmem_bknd_enabled = 0;
+	
+}
+
+module_init(kvm_host_tmem_bknd_init);
+module_exit(kvm_host_tmem_bknd_exit);
+
+/*END ABY MTP*/
diff -Nr -U 5 linux-4.1.3/drivers/staging/kvm_tmem_backend/Makefile patched_kernels/linux-4.1.3/drivers/staging/kvm_tmem_backend/Makefile
--- linux-4.1.3/drivers/staging/kvm_tmem_backend/Makefile	1970-01-01 05:30:00.000000000 +0530
+++ patched_kernels/linux-4.1.3/drivers/staging/kvm_tmem_backend/Makefile	2015-09-23 13:59:05.840216799 +0530
@@ -0,0 +1,4 @@
+#ABY MTP#
+obj-$(CONFIG_KVM_TMEM_HOST) += kvm_tmem.o 
+#obj-$(CONFIG_KTB_MAIN) += ktb_main.o
+#END ABY MTP#
diff -Nr -U 5 linux-4.1.3/include/linux/tmem.h patched_kernels/linux-4.1.3/include/linux/tmem.h
--- linux-4.1.3/include/linux/tmem.h	1970-01-01 05:30:00.000000000 +0530
+++ patched_kernels/linux-4.1.3/include/linux/tmem.h	2015-09-29 21:44:30.188445932 +0530
@@ -0,0 +1,144 @@
+#ifndef __TMEM_H_ 
+#define __TMEM_H_
+
+//#include <linux/rbtree.h> 
+//#include <linux/radix-tree.h> 
+
+#define TMEM_CONTROL               0
+#define TMEM_NEW_POOL              1
+#define TMEM_DESTROY_POOL          2
+#define TMEM_NEW_PAGE              3
+#define TMEM_PUT_PAGE              4
+#define TMEM_GET_PAGE              5
+#define TMEM_FLUSH_PAGE            6
+#define TMEM_FLUSH_OBJECT          7
+#define TMEM_READ                  8
+#define TMEM_WRITE                 9
+#define TMEM_XCHG                 10
+
+/* Bits for HYPERVISOR_tmem_op(TMEM_NEW_POOL) */
+#define TMEM_POOL_PERSIST          1
+#define TMEM_POOL_SHARED           2
+#define TMEM_POOL_PAGESIZE_SHIFT   4
+#define TMEM_POOL_PAGESIZE_MASK   0xf
+#define TMEM_POOL_VERSION_SHIFT   24
+#define TMEM_POOL_VERSION_MASK  0xff
+#define TMEM_VERSION_SHIFT        24
+
+
+/* flags for tmem_ops.new_pool */
+#define TMEM_POOL_PERSIST          1
+#define TMEM_POOL_SHARED           2
+#define KVM_TMEM_HCALL		  10
+
+struct kvm_tmem_op {
+        uint32_t cmd;
+        int32_t pool_id;
+        union {
+                struct {  /* for cmd == TMEM_NEW_POOL */
+                        uint64_t uuid[2];
+                        uint32_t flags;
+			uint16_t cli_id;
+                } new;
+                struct {
+                        uint64_t oid[3];
+                        uint32_t index;
+                        uint32_t tmem_offset;
+                        uint32_t pfn_offset;
+                        uint32_t len;
+                        unsigned long gfn; /* guest machine page frame */
+			uint16_t cli_id;
+                } gen;
+        } u;
+};
+
+/*ABY MTP*/
+
+/*********************************************/
+/*Declaration of tmem data structures and api*/ 
+/*********************************************/
+#define LOCAL_CLIENT ((uint16_t) - 1)
+#define TMEM_POOL_PRIVATE_UUID	{ 0, 0 }
+#define OBJ_HASH_BUCKETS 256
+//struct rb_root;
+
+struct tmem_pool_uuid {
+	u64 uuid_lo;
+	u64 uuid_hi;
+};
+
+struct tmem_oid {
+	u64 oid[3];
+};
+
+/*
+struct tmem_pool {
+
+    //bool shared;
+    //bool persistent;
+    bool is_dying;
+    void  *associated_client;
+    uint64_t uuid[2]; //0 for private, non-zero for shared 
+    uint32_t pool_id;
+    rwlock_t pool_rwlock;
+    atomic_t refcount;
+    struct rb_root obj_rb_root[OBJ_HASH_BUCKETS]; // protected by pool_rwlock
+    long obj_count;  // atomicity depends on pool_rwlock held for write
+    unsigned long objnode_count, objnode_count_max;
+    //struct list_head persistent_page_list;
+    long obj_count_max;  
+    struct tmem_page_descriptor *cur_pgp;
+
+};
+
+struct tmem_object_root {
+    struct tmem_oid oid;
+    struct rb_node rb_tree_node; //protected by pool->pool_rwlock
+    unsigned long objnode_count; //atomicity depends on obj_spinlock
+    long pgp_count; //atomicity depends on obj_spinlock
+    struct radix_tree_root tree_root; //tree of pages within object
+    struct tmem_pool* pool;
+    uint16_t last_client;
+    spinlock_t obj_spinlock;
+};
+
+extern void tmem_new_pool(struct tmem_pool* , uint32_t );
+extern int tmem_destroy_pool(struct tmem_pool*);
+*/
+
+
+/************************************************/
+/*Declaration of kvm host tmem backend functions*/
+/************************************************/
+
+/*Function that handles/parses the guest tmem op requests in kvm host*/
+//int kvm_host_tmem_op(/*struct kvm_vcpu *vcpu, struct tmem_oid *oid, */struct kvm_tmem_op *op, unsigned long *ret);
+//#define KVM_TMEM_NO_POOL -1
+//#define KVM_TMEM_NO_BACKEND -2
+
+extern int use_cleancache;
+extern int kvm_tmem_bknd_enabled;
+extern int kvm_tmem_dedup_enabled;
+
+struct kvm_host_tmem_ops {
+	unsigned long (*kvm_host_new_pool)(uint16_t, uint64_t, uint64_t, uint32_t);
+	unsigned long (*kvm_host_destroy_pool)(uint16_t, uint32_t);
+	unsigned long (*kvm_host_put_page)(uint16_t, uint32_t, struct tmem_oid* , uint32_t, struct page*);
+};
+
+extern void kvm_host_tmem_register_ops(struct kvm_host_tmem_ops *ops);
+/* extern int kvm_host_tmem_handler(struct kvm_vcpu* vcpu, struct kvm_tmem_op* op, unsigned long* ret); */
+extern int kvm_host_tmem_handler(struct kvm_vcpu* vcpu, gpa_t, unsigned long* ret); 
+
+/*Declaration of stubs to functions that service guest tmem ops in kvm host.*/
+
+/*
+ * extern void kvm_host_serv_new_page_request
+ * extern void kvm_host_serv_put_page_request
+ * extern void kvm_host_serv_get_page_request
+ * extern void kvm_host_serv_flush_page_request
+ * extern void kvm_host_serv_flush_object_request
+ */
+/*END ABY MTP*/
+
+#endif
